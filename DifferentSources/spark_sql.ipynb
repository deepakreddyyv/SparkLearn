{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52ac2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45d58750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 12:24:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.sql.warehouse.dir\", \"/home/deepak/programs/python/sparkLearn/spark-warehouse\").\n",
    "    config(\"javax.jdo.option.ConnectionURL\", \"jdbc:derby:/home/deepak/programs/python/sparkLearn/metastore_db;create=true\").\n",
    "    config(\"spark.sql.catalogImplementation\", \"hive\").\n",
    "    enableHiveSupport().\n",
    "    appName('SQL').\n",
    "    getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2c8251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ spark.sql.warehouse.dir\n",
    "# This sets the **location where Spark stores managed table data**.\n",
    "# Managed tables are created using CREATE TABLE without an explicit LOCATION.\n",
    "# Example:\n",
    "#   If you run \"CREATE TABLE demo (id INT)\", the table's data files\n",
    "#   will be stored inside this directory.\n",
    "# warehouse_dir = \"/home/deepak/programs/python/sparkLearn/DifferentSources/spark-warehouse\"\n",
    "\n",
    "# 2️⃣ javax.jdo.option.ConnectionURL\n",
    "# This tells Spark **where the Hive metastore database is located**.\n",
    "# Spark uses Derby (embedded DB) if no external metastore is configured.\n",
    "# By pointing it to a path, Spark persists **table metadata** across sessions.\n",
    "# \"create=true\" ensures the metastore is created if it doesn't exist.\n",
    "# metastore_url = \"jdbc:derby:/home/deepak/programs/python/sparkLearn/metastore_db;create=true\"\n",
    "\n",
    "# 3️⃣ spark.sql.catalogImplementation\n",
    "# This specifies the **catalog type Spark will use**.\n",
    "# Options:\n",
    "#   - \"in-memory\": default temporary catalog, disappears when session ends\n",
    "#   - \"hive\": uses a Hive metastore to persist table metadata\n",
    "# Setting this to \"hive\" ensures tables are persistent across sessions.\n",
    "# catalog_impl = \"hive\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dec7724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "show catalogs\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa2a914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 12:04:36 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/10/05 12:04:36 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore deepak@127.0.1.1\n",
      "25/10/05 12:04:36 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "show databases in spark_catalog\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e975496a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "show tables in default\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38c9336e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hive'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.catalogImplementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "827a38ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('drop table flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08406f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 12:15:01 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider JSON. Persisting data source table `spark_catalog`.`default`.`flights` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE spark_catalog.default.flights (\n",
    "DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count\n",
    "LONG)\n",
    "USING JSON OPTIONS (path '/home/deepak/programs/python/sparkLearn/data/flight-data/json/2015-summary.json')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f04a50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|  flights|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0294b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                           |comment|\n",
      "+----------------------------+------------------------------------------------------------------------------------+-------+\n",
      "|DEST_COUNTRY_NAME           |string                                                                              |NULL   |\n",
      "|ORIGIN_COUNTRY_NAME         |string                                                                              |NULL   |\n",
      "|count                       |bigint                                                                              |NULL   |\n",
      "|                            |                                                                                    |       |\n",
      "|# Detailed Table Information|                                                                                    |       |\n",
      "|Catalog                     |spark_catalog                                                                       |       |\n",
      "|Database                    |default                                                                             |       |\n",
      "|Table                       |flights                                                                             |       |\n",
      "|Owner                       |deepak                                                                              |       |\n",
      "|Created Time                |Sun Oct 05 12:15:01 IST 2025                                                        |       |\n",
      "|Last Access                 |UNKNOWN                                                                             |       |\n",
      "|Created By                  |Spark 4.0.1                                                                         |       |\n",
      "|Type                        |EXTERNAL                                                                            |       |\n",
      "|Provider                    |JSON                                                                                |       |\n",
      "|Location                    |file:/home/deepak/programs/python/sparkLearn/data/flight-data/json/2015-summary.json|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                  |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                                    |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat                           |       |\n",
      "+----------------------------+------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE EXTENDED FLIGHTS').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b24939d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 12:22:51 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`default`.`flights_csv` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    create table flights_csv (\n",
    "        DEST_COUNTRY_NAME STRING,\n",
    "        ORIGIN_COUNTRY_NAME STRING COMMENT \"remember, the US will be most prevalent\",\n",
    "        count LONG\n",
    "    ) using csv options (header true, path '/home/deepak/programs/python/sparkLearn/data/flight-data/csv')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "985bde88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------+---------------------------------------+\n",
      "|col_name                    |data_type                                                        |comment                                |\n",
      "+----------------------------+-----------------------------------------------------------------+---------------------------------------+\n",
      "|DEST_COUNTRY_NAME           |string                                                           |NULL                                   |\n",
      "|ORIGIN_COUNTRY_NAME         |string                                                           |remember, the US will be most prevalent|\n",
      "|count                       |bigint                                                           |NULL                                   |\n",
      "|                            |                                                                 |                                       |\n",
      "|# Detailed Table Information|                                                                 |                                       |\n",
      "|Catalog                     |spark_catalog                                                    |                                       |\n",
      "|Database                    |default                                                          |                                       |\n",
      "|Table                       |flights_csv                                                      |                                       |\n",
      "|Owner                       |deepak                                                           |                                       |\n",
      "|Created Time                |Sun Oct 05 12:22:51 IST 2025                                     |                                       |\n",
      "|Last Access                 |UNKNOWN                                                          |                                       |\n",
      "|Created By                  |Spark 4.0.1                                                      |                                       |\n",
      "|Type                        |EXTERNAL                                                         |                                       |\n",
      "|Provider                    |csv                                                              |                                       |\n",
      "|Location                    |file:/home/deepak/programs/python/sparkLearn/data/flight-data/csv|                                       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe               |                                       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                 |                                       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat        |                                       |\n",
      "|Storage Properties          |[header=true]                                                    |                                       |\n",
      "+----------------------------+-----------------------------------------------------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE EXTENDED FLIGHTS_CSV').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "760686b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from flights_csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2583e0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    create table managed_table (\n",
    "        NAME STRING,\n",
    "        AGE INTEGER\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef855c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "insert into managed_table values ('deepak', 24)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94ff5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  NAME|AGE|\n",
      "+------+---+\n",
      "|deepak| 24|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from managed_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b90b6905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "insert into managed_table values ('Krish', 27)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7af7b057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  NAME|AGE|\n",
      "+------+---+\n",
      "|deepak| 24|\n",
      "| Krish| 27|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from managed_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aebfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-10-05 12:28:32.118\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[TABLE_OR_VIEW_NOT_FOUND] The table or view `history` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01\", \"context\": {\"errorClass\": \"TABLE_OR_VIEW_NOT_FOUND\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o106.sql.\\n: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `history` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 9;\\n'DescribeColumn 'managed_table, false, [info_name#335, info_value#336]\\n+- 'UnresolvedTableOrView [history], DESCRIBE TABLE, true\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:303)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\\n\\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:462)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:449)\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:467)\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:303)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 21 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/deepak/anaconda3/envs/ai_data/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/deepak/anaconda3/envs/ai_data/lib/python3.13/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `history` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 9;\n'DescribeColumn 'managed_table, false, [info_name#335, info_value#336]\n+- 'UnresolvedTableOrView [history], DESCRIBE TABLE, true\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdescribe history managed_table\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_data/lib/python3.13/site-packages/pyspark/sql/session.py:1810\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1806\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1807\u001b[39m             errorClass=\u001b[33m\"\u001b[39m\u001b[33mINVALID_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1808\u001b[39m             messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(args).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1809\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1810\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1811\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1812\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_data/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai_data/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `history` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 9;\n'DescribeColumn 'managed_table, false, [info_name#335, info_value#336]\n+- 'UnresolvedTableOrView [history], DESCRIBE TABLE, true\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe history managed_table').show() # this will not work because the default table it parquet not delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c39f60de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                  |comment|\n",
      "+----------------------------+-------------------------------------------------------------------------------------------+-------+\n",
      "|NAME                        |string                                                                                     |NULL   |\n",
      "|AGE                         |int                                                                                        |NULL   |\n",
      "|                            |                                                                                           |       |\n",
      "|# Detailed Table Information|                                                                                           |       |\n",
      "|Catalog                     |spark_catalog                                                                              |       |\n",
      "|Database                    |default                                                                                    |       |\n",
      "|Table                       |managed_table                                                                              |       |\n",
      "|Owner                       |deepak                                                                                     |       |\n",
      "|Created Time                |Sun Oct 05 12:25:21 IST 2025                                                               |       |\n",
      "|Last Access                 |UNKNOWN                                                                                    |       |\n",
      "|Created By                  |Spark 4.0.1                                                                                |       |\n",
      "|Type                        |MANAGED                                                                                    |       |\n",
      "|Provider                    |parquet                                                                                    |       |\n",
      "|Location                    |file:/home/deepak/programs/python/sparkLearn/DifferentSources/spark-warehouse/managed_table|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                                |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                              |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                             |       |\n",
      "+----------------------------+-------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    'DESCRIBE EXTENDED managed_table'\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46020122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 15:09:46 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`default`.`delta_managed_table` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    create table delta_managed_table (\n",
    "        NAME STRING,\n",
    "        AGE INTEGER\n",
    "    ) using csv\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "065c526f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 15:11:26 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`default`.`managed_table_csv` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    create table managed_table_csv (\n",
    "        NAME STRING,\n",
    "        AGE INTEGER\n",
    "    ) using csv\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "326ced2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "insert into managed_table_csv values ('deepak', 24)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4596065f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                      |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n",
      "|NAME                        |string                                                                                         |NULL   |\n",
      "|AGE                         |int                                                                                            |NULL   |\n",
      "|                            |                                                                                               |       |\n",
      "|# Detailed Table Information|                                                                                               |       |\n",
      "|Catalog                     |spark_catalog                                                                                  |       |\n",
      "|Database                    |default                                                                                        |       |\n",
      "|Table                       |managed_table_csv                                                                              |       |\n",
      "|Owner                       |deepak                                                                                         |       |\n",
      "|Created Time                |Sun Oct 05 15:11:28 IST 2025                                                                   |       |\n",
      "|Last Access                 |UNKNOWN                                                                                        |       |\n",
      "|Created By                  |Spark 4.0.1                                                                                    |       |\n",
      "|Type                        |MANAGED                                                                                        |       |\n",
      "|Provider                    |csv                                                                                            |       |\n",
      "|Location                    |file:/home/deepak/programs/python/sparkLearn/DifferentSources/spark-warehouse/managed_table_csv|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                             |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                                               |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat                                      |       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    'DESCRIBE EXTENDED managed_table_csv'\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d8c56e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
